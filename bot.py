import logging
import os
import random
import re
import requests
from urllib.parse import urlparse, urljoin
import feedparser
from bs4 import BeautifulSoup
from telegram import Update
from telegram.ext import ApplicationBuilder, CommandHandler, ContextTypes
from datetime import datetime, timedelta
from time import mktime

# â”€â”€ ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
BOT_TOKEN = "8197963395:AAFj_KzRxjfoe7CbLh_aRJq_L4zek1S0h_4"
CHANNEL = "@osetia_lenta"

SOURCES = [
    {"name": "15-Ğ¹ Ğ Ğ•Ğ“Ğ˜ĞĞ",       "url": "https://region15.ru/rss/",          "allow_media": True},
    {"name": "ĞĞ»Ğ°Ğ½Ğ¸Ñ Ğ¢Ğ’",          "url": "https://alaniatv.ru/novosti/feed/", "allow_media": True},
    {"name": "Bezformata Ğ¢Ğ¾Ğ¿",     "url": "https://vladikavkaz.bezformata.com/rsstop.xml", "allow_media": False},
]

DB = "posted.txt"
INTERVAL = 900           # 15 Ğ¼Ğ¸Ğ½ÑƒÑ‚
MAX_POSTS_PER_RUN = 1
ACTUALITY_HOURS = 48     # Ğ½Ğµ ÑÑ‚Ğ°Ñ€ÑˆĞµ 2 ÑÑƒÑ‚Ğ¾Ğº

logging.basicConfig(level=logging.INFO, format='%(asctime)s | %(levelname)-5s | %(message)s')
logger = logging.getLogger(__name__)

# â”€â”€ Ğ£Ñ‚Ğ¸Ğ»Ğ¸Ñ‚Ñ‹ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def normalize(url):
    try: p = urlparse(url); return f"{p.scheme}://{p.netloc}{p.path.rstrip('/')}".lower()
    except: return url.lower().strip()

def load_posted():
    return set(line.strip() for line in open(DB, encoding='utf-8')) if os.path.exists(DB) else set()

def save_posted(link):
    with open(DB, "a", encoding="utf-8") as f: f.write(link + "\n")

def clean_text(html):
    if not html: return ""
    soup = BeautifulSoup(html, "html.parser")
    for tag in soup(["script", "style", "iframe"]): tag.decompose()
    for p in soup.find_all("p"): p.replace_with(p.get_text(strip=True) + "\n\n")
    return re.sub(r'\n{3,}', '\n\n', soup.get_text(separator="\n").strip())

def highlight(text):
    for word in ["Ğ’Ğ»Ğ°Ğ´Ğ¸ĞºĞ°Ğ²ĞºĞ°Ğ·", "Ğ¡ĞµĞ²ĞµÑ€Ğ½Ğ°Ñ ĞÑĞµÑ‚Ğ¸Ñ", "ĞĞ»Ğ°Ğ½Ğ¸Ñ", "ĞÑĞµÑ‚Ğ¸Ñ", "Ğ”Ğ¢ĞŸ"]:
        text = re.sub(rf"(?i)\b{re.escape(word)}\b", r"<b>\g<0></b>", text)
    return text

def smart_truncate(text, threshold=100):
    if len(text) <= threshold: return text
    pos = text.find(".", threshold)
    return text[:pos + 1] if pos != -1 else text[:threshold]

def extract_text(entry):
    for field in [
        entry.get("content", [{}])[0].get("value", ""),
        entry.get("summary", ""),
        entry.get("description", "")
    ]:
        cleaned = clean_text(field)
        if len(cleaned.strip()) > 30: return cleaned
    return ""

def find_media(entry):
    # ĞŸÑ€Ğ¸Ğ¾Ñ€Ğ¸Ñ‚ĞµÑ‚: Ğ²Ğ¸Ğ´ĞµĞ¾ â†’ Ñ„Ğ¾Ñ‚Ğ¾ Ğ¸Ğ· RSS â†’ Ñ„Ğ¾Ñ‚Ğ¾ Ğ¸Ğ· ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹
    if hasattr(entry, "media_content") and entry.media_content:
        for m in entry.media_content:
            url = m.get("url")
            if url:
                if re.search(r"\.(mp4|m4v|mov|webm)$", url, re.I): return {"type": "video", "url": url}
                if re.search(r"\.(jpe?g|png|webp|gif)$", url, re.I): return {"type": "photo", "url": url}

    if hasattr(entry, "enclosures") and entry.enclosures:
        for enc in entry.enclosures:
            url = enc.get("href") or enc.get("url")
            if url:
                if re.search(r"\.(mp4|m4v|mov|webm)$", url, re.I): return {"type": "video", "url": url}
                if re.search(r"\.(jpe?g|png|webp|gif)$", url, re.I): return {"type": "photo", "url": url}

    # ĞŸĞ°Ñ€ÑĞ¸Ğ½Ğ³ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹ â€” ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğ¹ Ğ´Ğ»Ñ region15.ru
    link = getattr(entry, "link", None)
    if link:
        try:
            r = requests.get(link, timeout=15, headers={"User-Agent": "Mozilla/5.0"})
            r.raise_for_status()
            soup = BeautifulSoup(r.text, "html.parser")

            # Ğ¡Ğ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ´Ğ»Ñ region15.ru: Ğ¸Ñ‰ĞµĞ¼ Ğ² .entry-content Ğ¸Ğ»Ğ¸ .post-thumbnail
            candidates = soup.select(".entry-content img, .post-thumbnail img, article img, img.size-full, img.wp-post-image")
            for img in candidates:
                src = img.get("src") or img.get("data-src") or img.get("data-lazy-src")
                if src and re.search(r"\.(jpe?g|png|webp)$", src, re.I):
                    # Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€ Ğ½Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ĞºĞ°Ñ€Ñ‚Ğ¸Ğ½ĞºĞ¸ (Ğ¸Ğ³Ğ½Ğ¾Ñ€Ğ¸Ñ€ÑƒĞµĞ¼ Ğ»Ğ¾Ğ³Ğ¾Ñ‚Ğ¸Ğ¿Ñ‹/Ğ¼Ğ°Ğ»ĞµĞ½ÑŒĞºĞ¸Ğµ)
                    if "logo" not in src.lower() and "avatar" not in src.lower():
                        full_url = urljoin(link, src)
                        # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€ (Ğ¾Ğ¿Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾, ĞµÑĞ»Ğ¸ Ğ½ÑƒĞ¶Ğ½Ğ¾ â€” Ğ´Ğ¾Ğ±Ğ°Ğ²ÑŒÑ‚Ğµ requests.head)
                        return {"type": "photo", "url": full_url}
        except Exception as e:
            logger.debug(f"ĞŸĞ°Ñ€ÑĞ¸Ğ½Ğ³ ĞºĞ°Ñ€Ñ‚Ğ¸Ğ½ĞºĞ¸ Ğ½Ğµ ÑƒĞ´Ğ°Ğ»ÑÑ Ğ´Ğ»Ñ {link}: {e}")

    return None  # Ğ•ÑĞ»Ğ¸ Ğ½Ğ¸Ñ‡ĞµĞ³Ğ¾ â€” Ñ‚ĞµĞºÑÑ‚

def prepare_post(entry):
    title = (entry.title or "Ğ‘ĞµĞ· Ğ·Ğ°Ğ³Ğ¾Ğ»Ğ¾Ğ²ĞºĞ°").strip()
    text = extract_text(entry)
    text = highlight(text)
    preview = smart_truncate(text)

    emoji = random.choice("ğŸ“°ğŸ“¢ğŸ”¥âš¡ğŸ”ï¸ğŸš¨ğŸ“âœ¨ğŸ¥")
    message = f"{emoji} <b>{title}</b>\n\n{preview}\n\n<b>@osetia_lenta</b>"

    return message, title

def get_entry_date(entry):
    for field in ['published_parsed', 'updated_parsed', 'created_parsed']:
        parsed = getattr(entry, field, None)
        if parsed:
            try:
                return datetime.fromtimestamp(mktime(parsed))
            except:
                pass
    return datetime.now()

# â”€â”€ ĞÑĞ½Ğ¾Ğ²Ğ½Ğ°Ñ Ğ»Ğ¾Ğ³Ğ¸ĞºĞ° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
async def check_feeds(context: ContextTypes.DEFAULT_TYPE):
    posted = load_posted()
    all_new_entries = []

    for source in SOURCES:
        try:
            feed = feedparser.parse(requests.get(source["url"], timeout=12).content)

            for entry in feed.entries:
                link = getattr(entry, "link", None)
                if not link: continue

                norm_link = normalize(link)
                if norm_link in posted: continue

                pub_date = get_entry_date(entry)
                if pub_date < datetime.now() - timedelta(hours=ACTUALITY_HOURS):
                    continue

                all_new_entries.append((pub_date, entry, source))

        except Exception as e:
            logger.error(f"ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ° {source['name']}: {e}")

    all_new_entries.sort(key=lambda x: x[0], reverse=True)

    posted_count = 0
    for pub_date, entry, source in all_new_entries:
        if posted_count >= MAX_POSTS_PER_RUN:
            break

        text, title = prepare_post(entry)
        caption = text if len(text) <= 1024 else text[:1010] + "â€¦"

        media = find_media(entry) if source.get("allow_media", False) else None

        try:
            if media and media["type"] == "video":
                r = requests.get(media["url"], timeout=30, headers={"User-Agent": "Mozilla/5.0"})
                r.raise_for_status()
                await context.bot.send_video(CHANNEL, r.content, caption=caption, parse_mode="HTML", supports_streaming=True)
            elif media and media["type"] == "photo":
                r = requests.get(media["url"], timeout=25, headers={"User-Agent": "Mozilla/5.0"})
                r.raise_for_status()
                await context.bot.send_photo(CHANNEL, r.content, caption=caption, parse_mode="HTML")
            else:
                await context.bot.send_message(CHANNEL, caption, parse_mode="HTML")

            posted.add(normalize(entry.link))
            save_posted(normalize(entry.link))
            posted_count += 1
            logger.info(f"[{source['name']}] ĞĞ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ¾Ğ²Ğ°Ğ½Ğ¾: {title[:60]}... ({media['type'] if media else 'text'}) â†’ {pub_date}")

        except Exception as e:
            logger.error(f"ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ {source['name']}: {e}")
            await context.bot.send_message(CHANNEL, caption, parse_mode="HTML")

    if posted_count == 0:
        logger.info("ĞĞµÑ‚ ÑĞ²ĞµĞ¶Ğ¸Ñ… Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ² Ğ² Ñ†Ğ¸ĞºĞ»Ğµ")

async def start(update: Update, context: ContextTypes.DEFAULT_TYPE):
    await update.message.reply_text("Ğ‘Ğ¾Ñ‚ Ğ·Ğ°Ğ¿ÑƒÑ‰ĞµĞ½ â€¢ 1 Ğ¿Ğ¾ÑÑ‚ / 15 Ğ¼Ğ¸Ğ½")

def main():
    app = ApplicationBuilder().token(BOT_TOKEN).build()
    app.add_handler(CommandHandler("start", start))
    app.job_queue.run_repeating(check_feeds, interval=INTERVAL, first=10)
    logger.info("Ğ‘Ğ¾Ñ‚ Ğ·Ğ°Ğ¿ÑƒÑ‰ĞµĞ½")
    app.run_polling()

if __name__ == "__main__":
    main()
